RESULTS_PATH = "results"
EXP_PATH = "exp"
SIMPARAM_PATH = "simulation_parameters"


CHANNELS = ["minus", "plus"]
PROBER_SUFF = ["gamma", "beta", "expr"]
PROBER_STAT = ["theta", "read_model"]


RESULTS = expand("{path}/{file}", path = RESULTS_PATH, 
                 file = ["number_of_reads_in_real_data.txt", "time_and_memory_table.txt", "mapping_statistics_table.txt"])
#                         "grid_search_for_prior.txt"])


REAL = "dms_real"
SIM = "dms_sim"
RSEM_SIM = "rsem_sim"
SIM2 = "dms_sim2"
FAKE = "dms_sim_fake"


#### Count number of reads in (-) and (+) channels after adaptors are trimmed

rule count_reads:
     input: REAL_DATA
     output: RESULTS[0]
     run:
        shell("wc -l {input[0]} | python3 -c 'print(\"(-): {{:,}}\".format(int(input().split()[0]) // 4))' > {output}")
        shell("wc -l {input[1]} | python3 -c 'print(\"(+): {{:,}}\".format(int(input().split()[0]) // 4))' >> {output}")

#### Evaluate running time and memory usage

rule time_and_memory:
     input:
        TOOLS[2], TOOLS[5],
        REAL_DATA,
        REF_FILT
     output:
        expand("{path}/{name}.{suffix}", path = EXP_PATH, name = REAL, suffix = PROBER_SUFF),
        expand("{path}/{name}.stat/{name}_{channel}.{suffix}", path = EXP_PATH, name = REAL, channel = CHANNELS, suffix = PROBER_STAT),
        expand("{path}/{name}_{channel}.bam", path = EXP_PATH, name = REAL, channel = CHANNELS),
        expand("{path}/{name}.{suffix}", path = EXP_PATH, name = REAL, suffix = ["time", "bam.time", "mem"])
     threads: 40
     shell:
        "{PROBER} estimate --time --memory --bowtie --bowtie-path {input[0]} -p {threads} --output-bam --read-length 37 --size-selection-min 21 --size-selection-max 526 "
        "{REF_FILT_NAME} {EXP_PATH}/{REAL} --reads {input[2]} {input[3]}"

rule generate_time_and_memory_table:
     input:
        expand("{path}/{name}.{suffix}", path = EXP_PATH, name = REAL, suffix = ["time", "bam.time", "mem"])
     output:
        RESULTS[1]
     shell:
        "{RESULTS_PATH}/scripts/generate_time_memory_table {EXP_PATH}/{REAL} {RESULTS[1]}"

#### Collect mapping statistics

rule extract_mapping_statistics:
     input:
        "{}/{}_{{channel}}.bam".format(EXP_PATH, REAL),
        "{}/rRNA_list.txt".format(ANNO_PATH)
     output:
        "{}/{}_{{channel}}.stats.txt".format(EXP_PATH, REAL)
     shell:
        "{RESULTS_PATH}/scripts/collectAlignStat {input[0]} {input[1]} > {output}"

rule generate_mapping_statistics_table:
     input:
        expand("{path}/{name}_{channel}.stats.txt", path = EXP_PATH, name = REAL, channel = CHANNELS)
     output:
        RESULTS[2]
     shell:
        "{RESULTS_PATH}/scripts/generate_mapping_statistics_table {input} {output}"

#### Generating prior value grid search table

rule run_PROBer_for_grid_search:
     input:
        TOOLS[5],
        expand("{path}/{name}_{channel}.bam", path = EXP_PATH, name = REAL, channel = CHANNELS),
        REF_FILT
     output:
        "{path}/{name}_{{gamma}}_{{beta}}.logMAP".format(path = EXP_PATH, name = REAL)
     threads: 60
     shell:
        "{PROBER} estimate -p {threads} --read-length 37 --size-selection-min 21 --size-selection-max 526 " 
        "--gamma-init {wildcards.gamma} --beta-init {wildcards.beta} -q --output-logMAP " 
        "{REF_FILT_NAME} {EXP_PATH}/{REAL}_{wildcards.gamma}_{wildcards.beta} --bam {input[1]} {input[2]}"

#rule generate_grid_search_table:
#     input:
#        expand("{path}/{name}_{gamma}_{beta}.logMAP", path = EXP_PATH, name = REAL, gamma = ["0.0001", "0.001", "0.01", "0.1"], beta = ["0.0001", "0.001", "0.01", "0.1"])
#     output:
#        RESULTS[3]
#     shell:
#        "{RESULTS_PATH}/scripts/generate_grid_search_table {EXP_PATH}/{REAL} {output}"

#########

#########

rule bowtie_alignment_all:
     input: TOOLS[1], TOOLS[2],
            "{sample_name}_{channel}_1.fq",
            REF_FILT, REF_SPIKE
     output: "{sample_name}_{channel}.bam",
             "{sample_name}_{channel}.err"
     threads: 30
     run:
        if wildcards.sample_name.startswith("sim_spike"):
           shell("{input[1]}/bowtie --norc -v 3 -a -m 200 -S -p {threads} {REF_SPIKE_NAME} {input[2]} 2> {output[1]} | "
                 "{input[0]}/samtools view -b -S -o {output[0]} -")
        else:
           shell("{input[1]}/bowtie --norc -v 3 -a -m 200 -S -p {threads} {REF_FILT_NAME} {input[2]} 2> {output[1]} | "
                 "{input[0]}/samtools view -b -S -o {output[0]} -")

rule run_PROBer:
     input: TOOLS[5],
            expand("{{sample_name}}_{channel}.bam", channel = CHANNELS),
            REF_FILT, REF_SPIKE
     output: expand("{{sample_name}}.{suffix}", suffix = PROBER_SUFF)
     threads: 30
     run:
        if wildcards.sample_name.startswith("sim_spike"):
           shell("{PROBER} estimate -p {threads} --read-length 37 --size-selection-min 21 --size-selection-max 526 "
                 "{REF_SPIKE_NAME} {wildcards.sample_name}_SE --bam {input[1]} {input[2]}")
        else:
           shell("{PROBER} estimate -p {threads} --read-length 37 --size-selection-min 21 --size-selection-max 526 "
                 "{REF_FILT_NAME} {wildcards.sample_name}_SE --bam {input[1]} {input[2]}")

rule run_RSEM_Spats_start_pipeline:
     input: TOOLS[6],
            expand("{{sample_name}}_{channel}.bam", channel = CHANNELS),
            "{SIMPARAM_PATH}/input_list_boxplot.txt",
            REF_FILT, REF_SPIKE
     output: expand("{{sample_name}}.{suffix}", suffix = ["gamma", "beta"])
     threads: 30
     run:
        if wildcards.sample_name.startswith("sim_spike"):
           shell("{input[0]}/PROBer-single-batch-estimate -p {threads} --input-list {input[3]} {REF_SPIKE_NAME} {wildcards.sample_name}_SE_RSEM "
                 "--bam {input[1]} {input[2]}")
        else:
           shell("{input[0]}/PROBer-single-batch-estimate -p {threads} --input-list {input[3]} {REF_FILT_NAME} {wildcards.sample_name}_SE_RSEM "
                 "--bam {input[1]} {input[2]}")

rule bowtie_alignment_best_strata:
     input: TOOLS[1], TOOLS[2],
            "{sample_name}_{channel}_1.fq",
            REF_FILT, REF_SPIKE
     output: "{sample_name}_{channel}_best_strata.bam",
             "{sample_name}_{channel}_best_strata.err"
     threads: 30
     run:
        if wildcards.sample_name.startswith("sim_spike"):
           shell("{input[1]}/bowtie --norc -a -v 3 --best --strata -S -p {threads} {REF_SPIKE_NAME} {input[2]} 2> {output[1]} | "
                 "{input[0]}/samtools view -b -S -o {output[0]} -")
        else:
           shell("{input[1]}/bowtie --norc -a -v 3 --best --strata -S -p {threads} {REF_FILT_NAME} {input[2]} 2> {output[1]} | "
                 "{input[0]}/samtools view -b -S -o {output[0]} -")


rule extract_count_vector_for_Ding_method:
     input: "{sample_name}_{channel}_best_strata.bam",
            expand("{ref_name}.grp", ref_name = [REF_FILT_NAME, REF_SPIKE_NAME])
     output: "{sample_name}_{channel}_best_strata.counts"
     run:
        if wildcards.sample_name.startswith("sim_spike"):
           shell("{RESULTS_PATH}/scripts/extractCountVectorsForDing {input[2]} {input[0]} {output}")
        else:
           shell("{RESULTS_PATH}/scripts/extractCountVectorsForDing {input[1]} {input[0]} {output}")

rule calculate_Ding_score:
     input: expand("{{sample_name}}_{channel}_best_strata.counts", channel = CHANNELS)
     output: "{sample_name}_Ding.scores"
     shell: "{RESULTS_PATH}/scripts/calcDing {input} {output}"

rule bowtie_alignment_best:
     input: TOOLS[1], TOOLS[2],
            "{sample_name}_{channel}_1.fq",
            REF_FILT, REF_SPIKE
     output: "{sample_name}_{channel}_best.bam",
             "{sample_name}_{channel}_best.err"
     threads: 30
     run:
        if wildcards.sample_name.startswith("sim_spike"):
           shell("{input[1]}/bowtie --norc -v 3 --best -S -p {threads} {REF_SPIKE_NAME} {input[2]} 2> {output[1]} | "
                 "{input[0]}/samtools view -b -S -o {output[0]} -")
        else:
           shell("{input[1]}/bowtie --norc -v 3 --best -S -p {threads} {REF_FILT_NAME} {input[2]} 2> {output[1]} | "
                 "{input[0]}/samtools view -b -S -o {output[0]} -")

rule extract_count_vector_for_Talkish_method:
     input: "{sample_name}_{channel}_best.bam"
     output: "{sample_name}_{channel}_best.counts"
     shell: "{RESULTS_PATH}/scripts/extractCountVectors {output} {input} all"

rule calculate_Talkish_score:
     input: expand("{{sample_name}}_{channel}_best.counts", channel = CHANNELS)
     output: "{sample_name}_Talkish.scores"
     shell: "{RESULTS_PATH}/scripts/calcTalkish {input} {output}"

rule generate_melt_file:
     input: expand("{ref_name}.{suffix}", ref_name = REF_FILT_NAME, suffix = ["mappability", "transcripts.fa"]),
            expand("{path}/sim_ground_truth.{suffix}", path = SIMPARAM_PATH, suffix = ["expr", "beta"]),
            expand("{{sample_name}}_{last_part}", last_part = ["SE.beta", "SE_RSEM.beta", "Ding.scores", "Talkish.scores"])
     output: "{sample_name}_results_melt.txt"
     shell: "{RESULTS_PATH}/scripts/analyzeResAC {wildcards.sample_name}_results plot 45 {input[2]} {input[0]} {input[1]} {input[3]} {input[4]} {input[5]} {input[6]} {input[7]}"

rule generate_box_plot:
     input: "dms_{name}_results_melt.txt"
     output: "{name}_boxplot.pdf"
     shell: "{RESULTS_PATH}/scripts/genBoxPlot {input} {output}"

 



            





          
     
